<html>
	<head>
		<style>
			html {
			background-color: black;
			color: white;
			font-family: sans-serif;
			}
			
			table, th, td {
			border: 1px solid white;
			border-collapse: collapse;
			}
			
			span.NoLineBreak {
				white-space: nowrap;
			}
			input[type='text'] { font-family: monospace;}
			input[type='number'] { font-family: monospace;}
			
			td label {   /* thanks to Patrick McElhaney - https://stackoverflow.com/questions/9739259/tick-a-checkbox-in-a-table-cell-by-clicking-anywhere-in-the-table-cell , causes table cells with labels inside them to focus onto whats inside a label */
				display: block;
				text-align: center;
			}
			
		</style>
	</head>
<body>
<h1 style="text-align: center;">General URL converter</h1>
<p>This is a general tool to convert, to generate multiple variations of, and to remove blacklisted URLs. Useful if you want to obtain URLs for full-resolution images, download links, etc. when you have URLs that are downsized version, a page linking to the download, etc.
As well as each page number of a paginated web page. This only works when websites have URL patterns or consistent format, that the URLs to be converted to don't have unknown substrings (different substring on each content) that is not being present in the URLs you are
converted from, such as randomly generated hashes, image URLs having an entirely different filename for full resolution compared to the downsized version, etc.</p>
<p>To edit, add, or remove converters, as well as add new key names to search or to filter, open this HTML file using a text editor (such as notepad++), and CTRL+F <kbd>ListOfConverters</kbd>. See the comments for more info.</p>

<p>Enter text that have URLs in them here</p>
<label><input type="checkbox" id="AutoUpdate" onchange="Auto_update()" checked>Auto update</label><br>
<textarea style="white-space: pre; overflow-wrap: normal; overflow-x: scroll;" id="Input_EnteredURLs" cols="100" rows="30" oninput="NotifyInputChanged(); Auto_update()"></textarea><br>
Pre-processing options:
<ul>
	<li><label><input type="checkbox" checked id="Checkbox_HTTPS" onchange="NotifyInputChanged(); Auto_update()">Force http &rarr; https</label></li>
</ul><br>

Filter:
<button onclick="ChangeFilterSettings('CheckboxFilters', 1); Auto_update()">Check all</button><button onclick="ChangeFilterSettings('CheckboxFilters', 0); Auto_update()">Uncheck all</button><button onclick="ChangeFilterSettings('CheckboxFilters', 2); Auto_update()">Invert</button>
<ul>
	<li><label><input type="checkbox" class="CheckboxFilters" checked onchange="Auto_update()" id="FilterSetting_converted">Converted URLs (replaced substrings)</label></li>
	<li><label><input type="checkbox" class="CheckboxFilters" checked onchange="Auto_update()" id="FilterSetting_pagination">pagination (have page number in URL)</label></li>
	<li><label><input type="checkbox" class="CheckboxFilters" onchange="Auto_update()" id="FilterSetting_blacklisted">Blacklisted (remove unwanted URLs such as ads, various google URLs, etc.)</label></li>
	<li><label><input type="checkbox" class="CheckboxFilters" checked onchange="Auto_update()" id="FilterSetting_other">other (URL not matching any of the listed patterns)</label></li>
</ul>
Keynames to filter:<button onclick="ChangeFilterSettings('CheckboxFiltersKeyName', 1); Auto_update()">Check all</button><button onclick="ChangeFilterSettings('CheckboxFiltersKeyName', 0); Auto_update()">Uncheck all</button><button onclick="ChangeFilterSettings('CheckboxFiltersKeyName', 2); Auto_update()">Invert</button>
<div style="overflow: auto; height: 300px; width: 300px; border: solid 1px; resize: both; white-space: nowrap;">
	<span id="Parent_ToggleableCustomFilters">
		<span id="ToggleableCustomFilters"></span>
	</span>
</div>
Sort:
<ul>
	<li><label><input type="radio" name="SortSetting" onchange="Auto_update()" id="SortSetting_None">None (be in the same order as the input, and also based on the converters)</label></li>
	<li><label><input type="radio" name="SortSetting" checked onchange="Auto_update()" id="SortSetting_ABC">Alphabetically</label> (<label><input type="radio" name="SortSetting" onchange="Auto_update()" id="SortSetting_ABC_Reverse">Reverse</label>)</li>
	<li><label><input type="radio" name="SortSetting" onchange="Auto_update()" id="SortSetting_IdentifiedType">By Identified type</label> (<label><input type="radio" name="SortSetting" onchange="Auto_update()" id="SortSetting_IdentifiedType_Reverse">Reverse</label>)</li>
	<li><label><input type="radio" name="SortSetting" onchange="Auto_update()" id="SortSetting_KeyName">By KeyName</label> (<label><input type="radio" name="SortSetting" onchange="Auto_update()" id="SortSetting_KeyName_Reverse">Reverse</label>)</li>
</ul>
<hr>
<kbd><span id="Output_URL_Count">0</span> URLs generated.</kbd><br>
<textarea style="white-space: pre; overflow-wrap: normal; overflow-x: scroll; background-color : #000000; color : #ffffff;" id="HTML_OutputString" cols="100" rows="30" readonly></textarea><br>
<button onclick="setClipboard(document.getElementById('HTML_OutputString').value)" id="Button_CopyOutput">Copy to clipboard</button><span id="CopiedTextMessage"></span><br><br><br>
<div style="overflow: scroll; height: 600px; border: solid 1px; resize: both;">
<span id="URL_OutputTable"></span>
</div>
<script>
	var SavedOutput = []
	var InputChanged = false
	function NotifyInputChanged() {
		InputChanged = true
	}
	//Nitter instance settings.
		//List of previously used nitter instances (as a reference to know which one is archivable), from most recent to oldest.
		//https://nitter.poast.org (503'ing)
		//https://nitter.moomoo.me (slow)
		//https://nitter.ktachibana.party
		//https://nitter.privacydev.net
		const Setting_Nitter_Instance_ConvertTo = "https://nitter.privacydev.net"
			//^Nitter instances:
			//-Have a protocol ("http://" or "https://", latter recommended)
			//-Have a hostname ("nitter.net")
			//Change this if you find that the current instance have issues. See a list of nitter instances here: https://github.com/xnaas/nitter-instances
			//Please note that media URLs may be configured differently, such as being base64Media encoded like this:
			//
			//https://<nitter_instance>/pic/orig/enc/<base64_string>
			//
			//If that is the case, then I would recommend you to extract links from nitter directly instead of grabbing links from twitter then converting
			//as the media URLs don't correlate between twitter and nitter.
			//
			//I strongly advise you to check that:
			// -Make sure you check if sensitive/age-restricted tweets are allowed to be displayed on nitter, if you're planning to save them. Find a tweet that
			//  is one of those types, take its URL and convert it to a nitter, and visit that URL. Proof:
			// --https://github.com/zedeus/nitter/issues/1040
			// --https://github.com/zedeus/nitter/issues/829
			//
			//Also, RIP pussthecat.org, some idiot UK police (presumably the City of London Police, the people who just make people guilty until proven innocent)
			//didn't properly check the service legitimacy and ended up flagging it as a pirate site, just because of an infringing content that was hosted on 
			//twitter and was passed through nitter. Nitter is not to be conscripted into doing twitter's copyright enforcement that they have no control over.
			//And I am totally against making intermediary services who acts as proxy (a mere pass-through) liable the same way as sites that actually do host
			//content. It's like making browser companies (google, firefox, etc.) liable just because the user using it to download illegal material and
			//expecting enforcement at the browser level, assuming that these people expect nitter to block out flagged content posted on twitter.
			//
			//2023-12-10 note: I noticed that twitter's media section including on the search results now only display a grid of images instead of tweets
			//until the image is clicked, and that it only displays the first image of a tweet when there are multiple (including if its loaded in the HTML)
			//Therefore if you try to extract URLs, you'll only get the URL of the first image and the tweet.
		const Setting_Nitter_Instance_ConvertFrom = "https://nitter.moomoo.me"
			//^Same as above, but when you are scraping from nitter instance A, but get URLs from instance B.
		
		//Don't touch these. It corrects the instance URLs in the event your browser appends a slash symbol at the end of the domain name.
			const Nitter_Instance_ConvertTo = Setting_Nitter_Instance_ConvertTo.replace(/\/$/, "")
			const Nitter_Instance_ConvertFrom = Setting_Nitter_Instance_ConvertFrom.replace(/\/$/, "")
	//Github stuff
		//This is a string to be RegExp()'ed into a regex. Needed to determine if the part of the URL is a username associated with a repository.
		//On notepad++ to convert, use ReplaceAll on:
		//- "\|" with "\n" to convert the vertical-bar-separated words into a list
		//- "[\r\n]" with "|" to convert it into  vertical-bar-separated words.
		//It must be "(?:(?!(?:" + <vertical-bar-separated words> + "))[A-Za-z0-9\\-]+)" to ensure no capturing groups.
		const Github_UsernamePart = "(?:(?!(?:about|apps|codespaces|collections|contact|customer\\-stories|enterprise|features|git\\-guides|images|login|mobile|organizations|orgs|premium-support|pricing|readme|search|security|signup|sitemap|solutions|sponsors|team|topics|trending|users))[A-Za-z0-9\\-]+)"
	//Bluesky stuff
		//Bluesky instances (URL format as far as I know:
		//https://bsky.app/profile/Username.bsky.social )
		//https://bsky.app/profile/Username.bsky.social/post/<hash>
			const BSkyDomainName = "(?:https:\\/\\/(?:bsky\\.app))"
			const UsernameHandle = "[^\\/\\.]+"
			const BSkyInstances = "(?:bsky\\.social)"
			const BskyPostHash = "[a-zA-Z\\d]+"
		//https://cdn.bsky.app/img/feed_thumbnail/plain/did:plc:<hash1>/<hash2>4@jpeg
		//https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:<hash1>/<hash2>@jpeg
		//https://cdn.bsky.app/img/avatar/plain/did:plc:<hash1>/<hash2>@jpeg
			const BSkyDomainName_CDN_Domain = "(?:https:\\/\\/(?:cdn.bsky.app))"
	//An array containing a list of objects specifying the replacement.
	//-When "Type" is "normal": performs a simple replacement (replace a substring or remove a part of the string), the attributes are:
	//--FindWhat: <Regex>: A regular expression the URLs in the input must match with to identify what appropriate replacement to perform a URL conversion.
	//  NOTE: I recommend matching the entire string (you may have to use ".+$" or ".*$") if you do not fully understand how the URL format of a given website work. I've
	//  experienced where the converted URL wasn't included in the output at all. So make sure as you add more objects, test every one of them!
	//--ReplaceWith: An array containing n objects, those objects are formatted: {Replacement:"https://example.com", KeyName:""}. Note: if this attribute omitted, will not include
	//  the matched URL at all. The KeyName attribute is optional.
	//-When "Type" is "pagination": This will generate each URL of each page number of the same kind, based on the highest page number of the matching kind that exists in the input,
	// Example, if you provide similar URLs with the difference being the page numbers:
	//--https://example.com/test?page=1 (usually "?page=1" is omitted but still works)
	//--https://example.com/test?page=4
	//--https://example.com/test?page=10
	//
	//  It will generate from page 1 to page 10. If you also have a similar thing but different numbers and have "test" be a different string, it will
	//  generate all of them each with their OWN (no mixup, string before and after the page number must match) maximum numbers.
	//--FindWhat: <Regex>: Same as above, but must match only the page number (in addition to making sure the URL pattern match), as string.split is being used.
	//---Make sure you use non-capturing group "(?:noncapturing)") since the array it split into (string.split) should only contain URL string before and after the number.
	//---Must use in a way that it only matches the number in the substring, it must avoid matching/including any characters that aren't page numbers (else an error occurs).
	//--PageNumberStartsAt: Page number to start at from counting from that number to the highest number. Some sites may have their page number start at 0, some at 1. Please
	//  let me know if you found pages that uses negative numbers (I never seen websites ever use negative page numbers), or fork this with the adoption of negative values.
	//--KeyName does the same thing as "normal" except that you have to have this be placed on the main object converter instead of an object inside an array inside a main
	//  object converter. This will apply on each page number of the URL.
	//
	//  Protip(s):
	//---Some sites may not display the final page number to jump to when there are a large enough number of pages (only shows the few page numbers around the page number
	//   you're currently on). If this is the case, you may be lucky that if the site shows the non-rounded total number of individual posts, and have a fixed number of post
	//   per page at maximum, you can find out the last page via mathematics: LastPageNumber = Math.ceil(TotalPosts/MaxPostPerPage). For example, if the user have 123 posts,
	//   and is paginated at a maximum of 50 posts per page, then the last page number is 3 because ceiling(123/50) = 3; first page is 1st to 50th posts containing 50 posts,
	//   second is 51st to 100th also 50, and the 3rd and last is 101st to 123rd containing a remainder of 23 posts. I have several "utility" userscripts that does that
	//   (such as github). Feel free to copy, make changes or create new ones for other websites.
	//----If the page number starts at 0 instead of 1, and still increments by 1, then it is Math.ceil(TotalPosts/MaxPostPerPage)-1, since the page number ranging is
	//    basically shifted by -1.
	//---If you want to match a URL string up to but not including a specific substring ("NotInclude") and everything after, use regex:
	//   (https:\/\/example\.com(?:(?!NotInclude).)*).*$
	//   credit to Tim Pietzcker - https://stackoverflow.com/a/3850095/11030779
	//
	//-When "Type" is "blacklisted", it can be filtered (filter setting). Only "FindWhat" is needed, anything that matches this regex will be labeled as blacklisted. Please note
	// if a URL matches on matches multiple listed objects with at least one of them not being blacklisted, it will be included.
	//
	//The "KeyName" is useful for sorting and filtering, such as to find pagination URLs to extract links from or to remove non-archivable pages. They're automatically added to
	//the filterable list when the HTML loads.
	//
	//See quick reference here: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions/Cheatsheet
		const ListOfConverters = [
//			{
//				//Example 1: Twitter image URLs
//				//https://pbs.twimg.com/media/CBAoaU1UwAIUPIc?format=jpg -> https://pbs.twimg.com/media/CBAoaU1UwAIUPIc?format=jpg&name=orig (converts modern to modern, so if you entered a modern, can be filtered)
//				Type:"normal",
//				FindWhat:/^https:\/\/pbs\.twimg\.com\/media\/([^\s\?]+\?format=[a-zA-Z0-9]+).*$/, //If this doesn't match the entire string, the substring that is not matched will be included, making the URL invalid
//				ReplaceWith:[{Replacement:"https://pbs.twimg.com/media/$1&name=orig", KeyName:"Twitter_Images_Modern_Orig"}, {Replacement:"https://pbs.twimg.com/media/$1&name=thumb", KeyName:"Twitter_Images_Modern_thumb"}]
//			},
//			{
//				//Example 1.1 Same as above but also converts old to modern (https://pbs.twimg.com/media/CBAoaU1UwAIUPIc.jpg -> https://pbs.twimg.com/media/CBAoaU1UwAIUPIc?format=jpg&name=orig)
//				Type:"normal",
//				FindWhat:/https:\/\/pbs\.twimg.com\/media\/([^\s\?]+)\.\.([a-zA-Z0-9]+)/,
//				ReplaceWith:[{Replacement:"https://pbs.twimg.com/media/$1.$2:orig", KeyName:"Twitter_Images_Modern_Orig"}, {Replacement:"https://pbs.twimg.com/media/$1.$2:thumb", KeyName:"Twitter_Images_Modern_thumb"}],
//			},
//			{
//				//Example 1.2: Opposite of example 1
//				//Example test: https://pbs.twimg.com/media/CBAoaU1UwAIUPIc?format=jpg&name=orig -> https://pbs.twimg.com/media/CBAoaU1UwAIUPIc.jpg:orig (converts old to old, so if you entered a modern, can be filtered)
//				Type:"normal",
//				FindWhat:/^https:\/\/pbs\.twimg\.com\/media\/([^\s\?]+)\.([a-zA-Z0-9]+).*$/, //If this doesn't match the entire string, the substring that is not matched will be included, making the URL invalid
//				ReplaceWith:[{Replacement:"https://pbs.twimg.com/media/$1.$2:orig", KeyName:"Twitter_Images_Old_Orig"}, {Replacement:"https://pbs.twimg.com/media/$1.$2:thumb", KeyName:"Twitter_Images_Old_Thumb"}]
//			},
//			{
//				//Example 1.3: Opposite of example 1.1, converts modern to old
//				//Example test: https://pbs.twimg.com/media/CBAoaU1UwAIUPIc?format=jpg&name=orig -> https://pbs.twimg.com/media/CBAoaU1UwAIUPIc.jpg:orig
//				Type:"normal",
//				FindWhat:/^https:\/\/pbs\.twimg\.com\/media\/([^\s\?]+)\?format=([a-zA-Z0-9]+).*$/, //If this doesn't match the entire string, the substring that is not matched will be included, making the URL invalid
//				ReplaceWith:[{Replacement:"https://pbs.twimg.com/media/$1.$2:orig", KeyName:"Twitter_Images_Old_Orig"}, {Replacement:"https://pbs.twimg.com/media/$1.$2:thumb", KeyName:"Twitter_Images_Old_Thumb"}]
//			},
			{
				//Example 2: Pagination test (made-up example)
				Type:"pagination",
				FindWhat:/(?:(?<=^https:\/\/example\.com\/[a-zA-Z0-9_]+\?page=))\d+(?:(?=.*$))/,
				PageNumberStartsAt:1,
				KeyName:"Example_Pagination"
			},
			{
				//Example 3: Blacklisted test (several google ad-related pages)
				Type:"blacklisted",
				FindWhat:/http(s)?:\/\/(aa|about|accounts|adservice|apis|ads|mail|myactivity|ogs|policies|store|sustainability|www)?\.google(\.com)?(\/.*)?$/,
				KeyName:"Garbage_AdPages"
			},
			{
				Type:"Blacklisted",
				FindWhat:/https:\/\/googleads.*$/,
				KeyName:"Garbage_AdPages"
			},
			{
				Type:"normal",
				FindWhat:/https:\/\/dropbox\.com\/s.*$/,
				KeyName:"Unsavable"
			},
			//Nitter pages
			//https://nitter.privacydev.net/pic/media%2FFtWxZRraYAMrvmM.jpg%3Fname%3Dsmall%26format%3Dwebp <- downsized image
			//https://nitter.moomoo.me/pic/orig/media%2FFtWxZRraYAMrvmM.jpg
			//https://pbs.twimg.com/media/FtWxZRraYAMrvmM?format=jpg&name=orig
			//https://nitter.ktachibana.party
			{
				Type:"blacklisted",
				FindWhat:RegExp("(" + (NitterInstance_StringToRegex(Nitter_Instance_ConvertTo) + "|" + NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom)) + ")\\/(pic\\/media.*name%3Dsmall|search|[a-zA-Z0-9\\-_]+\\/status\\/\\d+\\/(favoriters|retweeters)|about$|logo\\.png$|search$|settings)")
			},
			{
				//We don't want the home page consisting just a search bar.
				Type:"blacklisted",
				FindWhat:RegExp("^(" + (NitterInstance_StringToRegex(Nitter_Instance_ConvertTo) + "|" + NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom)) + ")\\/?$"),
			},
			{
				Type:"blacklisted",
				//FindWhat:/https:\/\/liberapay\.com\/zedeus.*$/
				FindWhat:RegExp("(" + (NitterInstance_StringToRegex(Nitter_Instance_ConvertTo) + "|" + NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom)) + ")" + "\/zedeus.*$")
			},
			{
				//Profile pages
				Type:"normal",
				FindWhat:RegExp("(?:" + NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom) + "|https:\\/\\/twitter\\.com|https?:\\/\\/(?:www\\.)?x\\.com)" + "\\/((?!(about|intent|logo\\.png|search|settings|pic))[a-zA-Z0-9\\-_]+).*$"),
				ReplaceWith:[{Replacement:Nitter_Instance_ConvertTo + "/$1", KeyName:"Nitter_UserProfile_Front"}, {Replacement:Nitter_Instance_ConvertTo + "/$1/with_replies", KeyName:"Nitter_UserProfile_Replies"}, {Replacement:Nitter_Instance_ConvertTo + "/$1/media", KeyName:"Nitter_UserProfile_Media"}]
			},
			{
				//Tweets
				Type:"normal",
				FindWhat:RegExp("(?:" + NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom) + "|https:\\/\\/twitter\\.com|https?:\\/\\/(?:www\\.)?x\\.com)" + "(\\/((?!(about|logo\\.png|search|pic))[a-zA-Z0-9\\-_]+)\\/status\\/\\d+).*$"),
				ReplaceWith:[{Replacement:Nitter_Instance_ConvertTo + "$1", KeyName:"Nitter_Tweet"}]
			},
			{
				//Images (convert nitter images to twitter modern images)
				Type:"normal",
				FindWhat:RegExp(NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom) + "\\/pic\\/orig\\/media%2F([a-zA-Z0-9\\-_]+)\\.([a-zA-Z0-9]+)$"),
				ReplaceWith:[{Replacement:"https://pbs.twimg.com/media/$1?format=$2&name=orig", KeyName:"Twitter_Images_Modern_Orig"},{Replacement:"https://pbs.twimg.com/media/$1?format=$2&name=thumb", KeyName:"Twitter_Images_Modern_thumb"}]
			},
			{
				//Yes a duplicate of above, so I have the nitter and twitter versions, and each with a different KeyName. Doing this just in case of a pesky rate limit
				//and that twitter image URLs seems to guarantee to always save unlike saving tweet pages (make sure nitter doesn't base-64 encode its media URLs though).
				
				//Nitter images (from a tweet that has pictures)
				Type:"normal",
				FindWhat:RegExp(NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom) + "(\\/pic\\/orig\\/media%2F[a-zA-Z0-9\\-_]+\\.[a-zA-Z0-9]+)$"),
				ReplaceWith:[{Replacement:Nitter_Instance_ConvertTo + "$1", KeyName:"Nitter_Images"}]
			},
			{
				//Profile images and banners
				Type:"normal",
				FindWhat:RegExp(NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom) + "(\\/pic\\/(https%3A%2F%2Fpbs\\.twimg\\.com%2Fprofile_banners%2F|pbs\\.twimg\\.com%2Fprofile_images%2F|profile_images%2F).*$)"),
				ReplaceWith:[{Replacement:Nitter_Instance_ConvertTo + "$1", KeyName:"Nitter_ProfileBannersAndImages"}]
			},
			{
				//Thumbnails on profile pages, converted to nitter images.
				//https://nitter.x86-64-unknown-linux-gnu.zip/pic/media%2FF4gd6gjakAAFhsD.jpg%3Athumb
				Type:"normal",
				FindWhat:RegExp(NitterInstance_StringToRegex(Nitter_Instance_ConvertFrom) + "\\/pic\\/media%2F([a-zA-Z0-9\\-_]+)\\.([a-zA-Z0-9]+).*$"),
				ReplaceWith:[{Replacement:Nitter_Instance_ConvertTo + "/pic/orig/media%2F$1.$2", KeyName:"Nitter_Images"}]
			},
			//Rid out links from nitter linking to non-savable twitter pages and https://liberapay.com/zedeus
			{
				Type:"blacklisted",
				FindWhat:/^(https?:\/\/twitter\.com\/[a-zA-Z0-9\-_]+\/(media|with_replies)|https:\/\/liberapay\.com\/zedeus\/?$)/
			},
			{
				Type:"blacklisted",
				FindWhat:/^https:\/\/twitter\.com\/search.*$/
			},
			//github pages
			//valid repository name: [A-Za-z0-9_.\-]+
			//https://github.com/Username
			//https://github.com/Username/RepositoryName
			//https://github.com/Username/RepositoryName/archive/refs/heads/master.zip
			//https://github.com/Username/RepositoryName/archive/refs/tags/filename.zip
			//https://github.com/Username/RepositoryName/releases/download/v01/filename.extension
			//https://github.com/Username?tab=repositories
			//https://github.com/orgs/UserName/repositories //Well-known organization when viewing the tabs of this user other than "overview"
			//https://github.com/Username?page=2&tab=repositories
			//https://github.com/Username/RepositoryName/releases?page=2
			//
			{
				//Repository download links
				Type:"normal",
				FindWhat:/(^https?:\/\/github\.com\/([A-Za-z0-9\-]+)\/([A-Za-z0-9_.\-]+)\/archive\/refs\/(?:(heads|tags))\/[^\/]+).*$/,
				ReplaceWith:[{Replacement:"$1", KeyName:"Github_Repository_Download"}, {Replacement:"https://github.com/$2", KeyName:"Github_Profile"}, {Replacement:"https://github.com/$2/$3", KeyName:"Github_Repository"}]
			},
			{
				//Github releases
				Type:"normal",
				FindWhat:/(https?:\/\/github\.com\/[A-Za-z0-9\-]+\/[A-Za-z0-9_.\-]+\/releases\/download\/.+$)/,
				ReplaceWith:[{Replacement:"$1", KeyName:"Github_Releases_Download"}]
			},
			{
				//Github repository
				//
				//Note: repositories may be empty, so a download link generated from "https://github.com/Username/RepositoryName" may instead open a
				//page with "This repository is empty." with a 401 error instead of bringing up the save as prompt to download.
				//Example: https://github.com/24qwert/riaa will generate various URLs, one of them is
				//https://github.com/24qwert/riaa/archive/refs/heads/master.zip which when opened, brings up that message instead of downloading. I
				//strongly recommend if you are planning to mass-save these (as well as the repository page itself) on WBGS, I strongly recommend
				//removing empty repositories using WBGS's "Check if URLs are available in the Live Web" feature, and filter out URLs with 401s (or
				//potentially any 4XX errors).
				Type:"normal",
				FindWhat: RegExp("^(((https?:\\/\\/github\\.com\\/" + Github_UsernamePart + ")\\/[A-Za-z0-9_.\\-]+).*$)"),
				ReplaceWith:[{Replacement:"$2", KeyName:"Github_Repository"}, {Replacement:"$2/releases", KeyName:"Github_Releases"}, {Replacement:"$2", KeyName:"Github_Profile"}, {Replacement:"$2/archive/refs/heads/master.zip", KeyName:"Github_Repository_Download"}]
			},
			{
				//Github user account
				Type:"normal",
				FindWhat: RegExp("(^https?:\\/\\/github\\.com\\/" + Github_UsernamePart + "$)"),
				ReplaceWith:[{Replacement:"$1", KeyName:"Github_Profile"}]
			},
			{
				//Github releases tag
				//https://github.com/UserName/RepositoryName/releases/tag/VersionName -> https://github.com/UserName/RepositoryName/releases/expanded_assets/VersionName
				Type:"normal",
				FindWhat: RegExp("^(https?:\\/\\/github\\.com\\/" + Github_UsernamePart + "\\/[A-Za-z0-9_.\\-]+\\/releases)\\/tag\\/([A-Za-z0-9_.\\-]+)"),
				ReplaceWith:[{Replacement:"$1/expanded_assets/$2", KeyName:"Github_ReleasesTag_DownloadPage"}]
			},
			{
				//Github issues
				//https://github.com/UserName/RepositoryName/issues?page=6&q=
				//https://github.com/UserName/RepositoryName/issues?page=2&q=is%3Aissue+is%3Aopen
				Type:"pagination",
				FindWhat: RegExp("(?:(?<=(?:^https?:\\/\\/github\\.com\\/" + Github_UsernamePart + "\\/[A-Za-z0-9_.\\-]+\\/issues\\?page=)))\\d+(.*$)"),
				PageNumberStartsAt:1,
				KeyName:"Github_Repository_IssueForum"
			},
			{
				//issues
				//https://github.com/UserName/RepositoryName/issues/12345
				//https://github.com/UserName/RepositoryName/issues/12
				Type:"normal",
				FindWhat: RegExp("^(https?:\\/\\/github\\.com\\/" + Github_UsernamePart + "\\/[A-Za-z0-9_.\\-]+\\/issues\\/\\d+).*$"),
				ReplaceWith:[{Replacement:"$1", KeyName:"Github_Repository_IssueThread"}]
			},
			{
				//Github repository list
				//https://github.com/UserName?page=123&tab=repositories
				//Github didn't provide a way to find the last page of a repository pagination, however, it does tell you how many total repositories and that there is a max of 30 repositories per page,
				//thus [NumberOfPaginationPages = max(ceiling(NumberOfRepositoriesTotal/30), 1)].
				Type:"pagination",
				FindWhat: RegExp("(?:(?<=(?:https?:\\/\\/github\\.com\\/" + Github_UsernamePart + "\\?page=)))\\d+(?=(?:&tab=repositories$))"),
				PageNumberStartsAt:1,
				KeyName:"Github_Repository_Lists"
			},
			{
				//Github repository list (organizations)
				//https://github.com/orgs/Username/repositories?page=2
				Type:"pagination",
				FindWhat: RegExp("(?:(?<=(?:https?:\\/\\/github\\.com\\/orgs\\/" + Github_UsernamePart + "\\/repositories\\?page=)))\\d+"),
				PageNumberStartsAt:1,
				KeyName:"Github_Repository_OrgsLists"
			},
			{
				//https://github.com/adam-p/markdown-here/releases?page=2
				Type:"pagination",
				FindWhat: RegExp("(?:(?<=(?:https?:\\/\\/github\\.com\\/" + Github_UsernamePart + "\\/[A-Za-z0-9_.\-]+\\/releases\\?page=)))\\d+(?:$)"),
				PageNumberStartsAt:1,
				KeyName:"Github_Repository_ReleasesList"
			},
			//Project bluesky (YAY it is open to public!!) Right now WBM have trouble saving them (white blank page)
			{
				//Userpage
				Type:"normal",
				FindWhat: RegExp("(" + BSkyDomainName + "\\/profile\\/" + UsernameHandle + "\\." + BSkyInstances + ").*$"),
				ReplaceWith:[{Replacement:"$1", KeyName:"Bsky_Userpage"}]
			},
			{
				//Post
				Type:"normal",
				FindWhat: RegExp("(" + BSkyDomainName + "\\/profile\\/" + UsernameHandle + "\\." + BSkyInstances + "\\/post\\/" + BskyPostHash + ")"),
				ReplaceWith:[{Replacement:"$1", KeyName:"Bsky_Post"}]
			},
			{
				//Images
				Type:"normal",
				FindWhat: RegExp("(" + BSkyDomainName_CDN_Domain + ")\\/img\\/feed_(?:thumbnail|fullsize)(\\/.*$)"),
				ReplaceWith:[{Replacement:"$1/img/feed_thumbnail$2",KeyName:"Bsky_Img_thumb"},{Replacement:"$1/img/feed_fullsize$2",KeyName:"Bsky_Img_FullSize"}]
			},
			{
				//Avatar
				Type:"normal",
				FindWhat: RegExp("(" + BSkyDomainName_CDN_Domain + "\\/img\\/avatar\\/plain\\/.*$)"),
				ReplaceWith:[{Replacement:"$1",KeyName:"Bsky_Avatar"}]
			}
		]
	window.onload = LoadCustomToggableFilters
	function LoadCustomToggableFilters() {
		let ListOfKeyNames = new Set()
		ListOfConverters.forEach((Converter) => {
			if (Converter.Type == "normal") {
				if (Converter.hasOwnProperty("ReplaceWith")) {
					Converter.ReplaceWith.forEach((ReplacementList) => {
						if (ReplacementList.hasOwnProperty("KeyName")) {
							ListOfKeyNames.add(ReplacementList.KeyName)
						}
					});
				}
			} else if (Converter.Type == "pagination"||Converter.Type == "blacklisted") {
				if (Converter.hasOwnProperty("KeyName")) {
					ListOfKeyNames.add(Converter.KeyName)
				}
			}
		});
		ListOfKeyNames = Array.from(ListOfKeyNames)
		ListOfKeyNames.sort() //We now have a list of keynames we can filter out, in Alphabetical order.
		
		let UnorderedListOfKeynameFilters = document.createElement("ul")
		ListOfKeyNames.forEach((ArrayElement) => {
			let Element_li = document.createElement("li")
			let Element_label = document.createElement("label")
			let Element_input = document.createElement("input")
			Element_input.setAttribute("type", "checkbox")
			Element_input.setAttribute("checked", "checked");
			Element_input.setAttribute("id", "FilterSetting_" + ArrayElement)
			Element_input.setAttribute("onchange", "Auto_update()")
			Element_input.setAttribute("class", "CheckboxFiltersKeyName")
			let Element_kbd = document.createElement("kbd")
			
			Element_li.appendChild(Element_label) //<li><label></label></li>
			Element_label.appendChild(Element_input) //<li><label><input type="checkbox"></label></li>
			Element_label.appendChild(Element_kbd) //<li><label><input type="checkbox"><kbd></kbd></label></li>
			Element_kbd.appendChild(document.createTextNode(ArrayElement)) //<li><label><input type="checkbox"><kbd>KeyName</kbd></label></li>
			
			UnorderedListOfKeynameFilters.appendChild(Element_li)
		});
		document.getElementById("Parent_ToggleableCustomFilters").insertBefore(UnorderedListOfKeynameFilters, document.getElementById("ToggleableCustomFilters"));
	}
	
	function NitterInstance_StringToRegex(StringInput) {
		return StringInput.replace("/", "\\/").replace(".", "\\.")
	}
	function Auto_update() {
		if (document.getElementById("AutoUpdate").checked) {
			MainFunction()
		}
	}
	function ChangeFilterSettings(ElementClass, Setting) {
		//ElementClass: the element class to be affected by this function
		//Setting:
		//0 = uncheck
		//1 = check
		//2+ = invert
		if (Setting < 2) {
			for (let i = 0; i < document.getElementsByClassName(ElementClass).length; i++) {
				document.getElementsByClassName(ElementClass)[i].checked = Setting
			}
		} else {
			for (let i = 0; i < document.getElementsByClassName(ElementClass).length; i++) {
				document.getElementsByClassName(ElementClass)[i].checked = document.getElementsByClassName(ElementClass)[i].checked ^ 1
			}
		}
	}
	//Copy text from textarea
		function setClipboard(String) {
			//Credit goes to Mozilla: https://developer.mozilla.org/en-US/docs/Web/API/Clipboard/write
			const type = "text/plain";
			const blob = new Blob([String], { type });
			const data = [new ClipboardItem({ [type]: blob })];
	
			navigator.clipboard.write(data).then(
				() => {
				/* success */
					document.getElementById("CopiedTextMessage").innerHTML = " <span style='color: #00FF00;'>Copied!</span>"
					setTimeout(DeleteCopyMessage, 1500)
				},
				() => {
				/* failure */
					document.getElementById("CopiedTextMessage").innerHTML = " <span style='color: #FF0000;'>Copy failed!</span>"
					setTimeout(DeleteCopyMessage, 1500)
				}
			);
		}
		function DeleteCopyMessage() {
			document.getElementById("CopiedTextMessage").innerHTML = ""
		}
	{
		let OutputSet = new Set() //Had to have set contain strings because if set contains any object and not primitive, would allow duplicates.
		let OutputURLObjects = [] //
		function MainFunction() {
			OutputSet = new Set() //Reset/clear them to not include previous versions
			OutputURLObjects = [] //
			document.getElementById("URL_OutputTable").innerHTML = ""
			if (InputChanged) {
				let ListOfURLs = document.getElementById("Input_EnteredURLs").value.match(/ttp(s)?\:\/\/(?!data:)[^\s\"\']+/g) //Why ttps instead of https? Blame firefox's URL truncation of long URLs.
				if (ListOfURLs == null) {
					ListOfURLs = [] //failsafe, prevents Array.map from performing a null which on the browser, would do nothing (if user enters a string with no valid URL).
				}
				ListOfURLs = ListOfURLs.filter((URL) => { //Remove URLs that aren't web pages
					return (!/^ttps?:\/\/(mailto|file|data|irc)/.test(URL))
				})
				
				ListOfURLs = ListOfURLs.map(ArrayElement => {
					let ConvertedURL = ArrayElement.replace(/^ttp(s)?/, "http$1").replace(/https?:\/\/web\.archive\.org\/web\/\d+(im_)?\//, "").replace(/#.*$/, "").replace(/\.$/, "")
					if (document.getElementById("Checkbox_HTTPS").checked) {
						ConvertedURL = ConvertedURL.replace(/^http:\/\//, "https://")
					}
					let URLObject = {
						URL:ConvertedURL,
							//^-Revert the URL to have the "h" of the http/https
							// -When extracting links on WBM version of pages, obtain the original version of URLs (ideal for saving outlinks from a volatile page).
							// -remove fragment identifier
							// -remove the erroneous dot at the end from the error message
						HaveBeenChecked:false, //This is used to check for duplicate or 2 URLs being similar (same URL but different page number)
						IdentifiedType:"",
						KeyName:"" //Failsafe - for blacklisted or IdentifiedType being "other", should have its KeyName be blank instead of "undefined"
					}
					return URLObject
				});
		
				//Some loops had to use a for loop instead of array.foreach because
				//-Not recommended for concurrent modification since we are checking for potential same/similar URL
				//-We check every possible combinations, so there are two index numbers with the second one starting at FirstIndex+1 instead of 0.
				for (let i = 0; i < ListOfURLs.length; i++) { //Loop through every entered URL by the user
					let UrlMatchedAnyConverter = false
					if (ListOfURLs[i].HaveBeenChecked == false) { //Skip pagination URLs that have been checked by inner loop (optimization purposes)
						ListOfConverters.forEach(Converter => { //For each URL the user entered, loop through converters
							if (Converter.FindWhat.test(ListOfURLs[i].URL)) { //If URL matches one or more of the converters (a URL gets to loop through multiple converters)
								UrlMatchedAnyConverter = true
								if (Converter.Type == "normal") {
									if (Converter.hasOwnProperty("ReplaceWith")) {
										Converter.ReplaceWith.forEach(ReplaceWithText => {
											let SetKeyName = ""
											if (ReplaceWithText.hasOwnProperty("KeyName")) {
												SetKeyName = ReplaceWithText.KeyName
											}
											//OutputSet.add(ListOfURLs[i].URL.replace(Converter.FindWhat, ReplaceWithText))
											AddToOutput({
												URL:ListOfURLs[i].URL.replace(Converter.FindWhat, ReplaceWithText.Replacement),
												IdentifiedType:"converted",
												KeyName:SetKeyName
											});
										});
									};
								} else if (Converter.Type == "pagination") {
									let SetKeyName = ""
									if (Converter.hasOwnProperty("KeyName")) {
										SetKeyName = Converter.KeyName
									}
									let HighestPageNumberSoFar = parseInt(ListOfURLs[i].URL.match(Converter.FindWhat)[0], 10) //Track highest page number starting with the number we first processed
									let PartsOfURLSame = ListOfURLs[i].URL.split(Converter.FindWhat) //Get substrings before and after the page number (must be: ["string before number", "string after number"], a length of 2)
									for (let i2 = i+1; i2 < ListOfURLs.length; i2++) { //Loop to find a potential same URL but with higher page number
										PartsOfURLSame2 = ListOfURLs[i2].URL.split(Converter.FindWhat)
										if (Converter.FindWhat.test(ListOfURLs[i2].URL) && (PartsOfURLSame[0] == PartsOfURLSame2[0]) && (PartsOfURLSame[1] == PartsOfURLSame2[1])) { //if regex and all parts of the URL the same ignoring page number, compare the page number
											HighestPageNumberSoFar = Math.max(HighestPageNumberSoFar, parseInt(ListOfURLs[i2].URL.match(Converter.FindWhat)[0], 10)) //Update highest page number
											ListOfURLs[i2].HaveBeenChecked = true //Don't check this again when the outer loop advances
										}
									}
									//We now have the maximum number based on the user input
									for (let i2 = Converter.PageNumberStartsAt; i2 <= HighestPageNumberSoFar; i2++) { //Generate a URL of each page number
										//OutputSet.add(PartsOfURLSame[0] + i2.toString(10) + PartsOfURLSame[1])
										AddToOutput({
											URL:PartsOfURLSame[0] + i2.toString(10) + PartsOfURLSame[1], //Insert each page number URL
											IdentifiedType:"pagination",
											KeyName:SetKeyName
										});
									}
								} else if (Converter.Type == "blacklisted") {
									ListOfURLs[i].IdentifiedType = "blacklisted"
									AddToOutput(ListOfURLs[i])
								}
							}
						});
						if ((UrlMatchedAnyConverter == false)&&document.getElementById("FilterSetting_other").checked) {
							//OutputSet.add(ListOfURLs[i].URL)
							ListOfURLs[i].IdentifiedType = "other"
							AddToOutput(ListOfURLs[i])
						}
					}
				}
				//Save finished output array into storage
				SavedOutput = OutputURLObjects
				InputChanged = false //Reset this back to false so that if input change again, this above code will run
			} else {
				//Load
				OutputURLObjects = SavedOutput
			}
			let Filtered_OutputURLObjects = OutputURLObjects.filter((ArrayElement) => {
				let IsIdentifiedTypeAllowed = document.getElementById("FilterSetting_" + ArrayElement.IdentifiedType).checked
				let IsKeynameAllowed = true
				if (document.getElementById("FilterSetting_" + ArrayElement.KeyName) != null) {
					IsKeynameAllowed = document.getElementById("FilterSetting_" + ArrayElement.KeyName).checked
				}
				return (IsIdentifiedTypeAllowed && IsKeynameAllowed)
			})
			
			document.getElementById("Output_URL_Count").innerHTML = BigInt(Filtered_OutputURLObjects.length).toString(10)
			
			if (document.getElementById("SortSetting_ABC").checked) {
				Filtered_OutputURLObjects.sort((a, b) => {
					if (a.URL === b.URL) {
						return 0;
					} else {
						return (a.URL < b.URL) ? -1 : 1;
					}
				});
			} else if (document.getElementById("SortSetting_ABC_Reverse").checked) {
				Filtered_OutputURLObjects.sort((a, b) => {
					if (a.URL === b.URL) {
						return 0;
					} else {
						return (a.URL < b.URL) ? 1 : -1;
					}
				});
			} else if (document.getElementById("SortSetting_IdentifiedType").checked) {
				Filtered_OutputURLObjects.sort((a, b) => {
					if (a.IdentifiedType === b.IdentifiedType) {
						return 0;
					} else {
						return (a.IdentifiedType < b.IdentifiedType) ? -1 : 1;
					}
				});
			} else if (document.getElementById("SortSetting_IdentifiedType_Reverse").checked) {
				Filtered_OutputURLObjects.sort((a, b) => {
					if (a.IdentifiedType === b.IdentifiedType) {
						return 0;
					} else {
						return (a.IdentifiedType < b.IdentifiedType) ? 1 : -1;
					}
				});
			} else if (document.getElementById("SortSetting_KeyName").checked) {
				Filtered_OutputURLObjects.sort((a, b) => {
					if (a.KeyName === b.KeyName) {
						return 0;
					} else {
						return (a.KeyName < b.KeyName) ? -1 : 1;
					}
				});
			} else if (document.getElementById("SortSetting_KeyName_Reverse").checked) {
				Filtered_OutputURLObjects.sort((a, b) => {
					if (a.KeyName === b.KeyName) {
						return 0;
					} else {
						return (a.KeyName < b.KeyName) ? 1 : -1;
					}
				});
			}
			let OutputString = ""
			
			Filtered_OutputURLObjects.forEach((ArrayElement, ArrayIndex, ArrayItself) => {
				OutputString += ArrayElement.URL
				if (ArrayIndex != ArrayItself.length - 1) {
					OutputString += "\n"
				}
			});
			document.getElementById("HTML_OutputString").value = OutputString
			
			let OutputHTMLTableString = ""
			OutputHTMLTableString += "<table>"
			OutputHTMLTableString += "<tr><th>URL</th><th>Identified Type</th><th>Key name</th></tr>"
			Filtered_OutputURLObjects.forEach((ArrayElement) => {
				OutputHTMLTableString += "<tr><td><kbd>"+escapeHTMLChars(ArrayElement.URL)+"</kbd></td><td><kbd>"+ArrayElement.IdentifiedType+"</kbd></td><td><kbd>"+ArrayElement.KeyName+"</kbd></td></tr>"
			});
			OutputHTMLTableString += "</table>"
			document.getElementById("URL_OutputTable").innerHTML = OutputHTMLTableString
		}
		function AddToOutput(URLObject) {
			if (!OutputSet.has(URLObject.URL)) {	//If output not have the URL, add it to the list
				OutputSet.add(URLObject.URL)
				OutputURLObjects.push(URLObject)
			} else {
				//If already has it on the list, but its IdentifiedType is "other", and/or KeyName is blank, update it
				//Situations like this happen if a given URL not matching with any converters came first, then
				//another URL after converted through processing happens to match the former URL. That later URL have
				//more information than the one already on the list gets prevented from being added, so we have a URL
				//with inferior data.
				//
				//example:
				//https://github.com/Username
				//https://github.com/Username/RepositoryName
				//https://github.com/Username/RepositoryName/archive/refs/heads/master.zip
				//
				//If there is only a converter that matches the 3rd URL (and converts into the first two), and the first two URLs are already in the set,
				//they'll be labeled as "other" with a blank keyname, and then when the third URL gets processed, and then converted into the first two
				//URLs, they won't be added, despite those having known information (IdentifiedType and KeyName)
				for (let i = 0, j=OutputURLObjects.length; i < j; i++) {
					if (URLObject.URL == OutputURLObjects[i].URL) { //Match is found, update any missing info in OutputURLObjects[i].URL
						if (OutputURLObjects[i].IdentifiedType == "other" && URLObject.IdentifiedType != "other") {
							OutputURLObjects[i].IdentifiedType = URLObject.IdentifiedType
						}
						if (OutputURLObjects[i].KeyName == "" && URLObject.KeyName != "") {
							OutputURLObjects[i].KeyName = URLObject.KeyName
						}
						break;
					}
				}
			}
		}
	}
	function escapeHTMLChars(string) {
		//Should the user's input contains "<", ">", and "&", prevent the innerHTML from treating these as HTML tags/escapes
			string = string.replaceAll("&", "&amp;") //This must be done first, to avoid replacing already-replaced symbol's ampersand character.
			string = string.replaceAll("<", "&lt;")
			string = string.replaceAll(">", "&gt;")
			string = string.replaceAll("\n", "<br>") //Again just in case if WBGS would ever use linebreaks in a cell.
			return string
	}
</script>