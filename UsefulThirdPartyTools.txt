Useful browser extensions (and other things besides browser extensions) to save pages to the Internet archive.
Extensions:
	Open multiple URLs
		#OpenAllUrlsListed #tab
		https://github.com/htrinter/Open-Multiple-URLs/
		
		Good for if you want to open a list of URLs all at once if you check them frequently. Such as saving twitter pages
		(use along with GoodTwitter). Don't forget to save the URLs as text onto your machine if you plan on using this
		extension for more than just IA-ing the pages.
	Copy All Urls
		#Tab
		https://chrome.google.com/webstore/detail/copy-all-urls/djdmadneanknadilpjiknlnanaolmbfk/related
		https://github.com/vincepare/CopyAllUrl_Chrome
		
		This is useful if you have pages that you HAVE to manually open since extensions to extract links don't work,
		therefore you open each link in a new tab.
	
	Link gopher
		Original:
			#Links #Extractor
			https://sites.google.com/site/linkgopher/
			https://github.com/az0/linkgopher/
			This is useful if the link extractor batch script (see here: https://github.com/devharsh/Links-Extractor ) cannot
			catch links generated by JS when the page loads (or that the page hosting the links are behind a login wall).
			This is very good for sites with infinite loading.
		Fork:
			https://github.com/andrewdbate/linkgopher/
			This one allows you to extract links on multiple tabs. Ideal if again, the link extractor batch script cannot
			extract links on multiple URLs given, and you have multiple places to extract. Make sure though that your browser
			keeps each tab loaded (tab discarding should be disabled, otherwise tabs will "unload" and the link extractor may
			fail to extract links on those tabs).
			
			Google chrome:
				For google chrome 75 and later, they did removed the option so this extension
				is needed: https://chrome.google.com/webstore/detail/disable-automatic-tab-dis/dnhngfnfolbmhgealdpolmhimnoliiok but
				be careful as the description says, that it has a chance to fail.
			Firefox:
				Go to the search bar, and type [about:config], accept the risk and continue, and on the "Search preference name"
				type "unloadOnLowMemory", and make sure it is set to [false].
			
			In the event all browsers wouldn't allow the user to disable tab discarding, you'll have to do N-tabs at a time where N
			is a number that is below a threshold (depends on how much memory used up on the pages in total) that causes the browser
			to unload tabs.
	
	GoodTwitter (don't use on and after june 1 2020! Use a greasemonkey script!)
		#Twitter #Layout #LoadedContent #OldTwitterLayout
		https://github.com/ZusorCode/GoodTwitter
		Due to the new twitter layout (unloads content when new content is loaded, URLs format on images, gifs, and other
		media content is different), Link gopher alone cannot catch ALL URLs on the page (only currently existing posts
		loaded and not unloaded), so using this extension will use the old layout and keeps loaded content from being
		unloaded, enabling you to catch all links that entered view.
		
		NOTE: During my testing, it is best not to set your default page to load as the startup when opening your browser
		because it will first load twitter using the modern layout, redirect you from
		https://twitter.com/<Username>/<area, such as media> to the homepage, because when the browser opens, it first
		loads the page before loading the extensions, a workaround is not having your default page on twitter, and
		use “Open multiple URLs” extension.
		
		To save images, firefox have a feature “view page info”, go to the media tab, and select all of the items, copy that
		you should have all the images. Remember that the format as follows:
		Images:
			https://pbs.twimg.com/media/<Base64String>.<FileExtension>:<size>
			<Base64String> acts as an ID
			<FileExtension> the extension (.jpg .png, etc.)
			<size>:
				[] (nothing) the downsized “medium” resolution
				[:orig] for the original resolution
				[:thumb] thumbnail - used when a tweet has multiple images (3+).
			GIFs and videos:
				Note that this doesn't work if the media is not stored on
				twitter (URL contains [ext_tw_video] which means "external tweet video")
				but linked from an external source, such as facebook, youtube, etc.
				
				Also will not work if the video is long enough that the actual file
				is containing only a portion of the media due to being a streaming
				type of media.
				
				URL formats:
					https://pbs.twimg.com/tweet_video_thumb/<Base64String>.<FileExtension>
					https://pbs.twimg.com/tweet_video/<Base64String>.mp4 - probably for gifs.
					https://video.twimg.com/tweet_video/<Base64String>.mp4
		To save links to a tweet, after getting links from “Link gopher” copy that, and look for URLs that contain
		this:
			https://twitter.com/<Username>/status/<ID digits>
	Greasemonkey/Tampermonkey scripts
		This allows almost no limitations on codes you can execute
		
		https://github.com/greasemonkey/greasemonkey/ (active)
		https://github.com/Tampermonkey/tampermonkey (inactive)
		
		Several useful codes:
			Extract twitter links and media links:
				https://greasyfork.org/en/forum/discussion/79273/extract-links-to-tweet-and-media-on-twitter-as-you-scroll-down
				
				The outputted URLs will show up in the devtools's console log. I highly recommend changing the log limit to some very large
				number as old logs may be deleted. This can be changed on firefox's [about:config] and setting [devtools.hud.loglimit.console]
				and [extensions.firebug.console.logLimit] to the largest value - 2147483647 (the highest signed 32-bit integer).
				
				Even better, if your browser lets you have a console log for all tabs, such as firefox “Browser Console”:
				https://firefox-source-docs.mozilla.org/devtools-user/browser_console/index.html , you can extract
				links even faster. CTRL+click/rightclick -> Open link in a new tab/Pressing the scroll wheel will help you.
	
	Save to the wayback machine
		https://chrome.google.com/webstore/detail/save-to-the-wayback-machi/eebpioaailbjojmdbmlpomfgijnlcemk
		https://github.com/VerifiedJoseph/Save-to-the-Wayback-Machine
		
		This is a tool useful for quick page-saving and also check if the URL have been saved.
	Percent encode/decode
		https://chrome.google.com/webstore/detail/url-decode-encode/dgoepmkoiphgabefpbapldnjmbbiaoag
		In case if you extract links from a page contains non-UTF characters.
		
	Internet archive extension
		https://help.archive.org/hc/en-us/articles/360001513491-Save-Pages-in-the-Wayback-Machine
		A shortcut to save URLs quickly without having to go to the home page, or type
		[https://web.archive.org/save/<URL to save here>].
	
	View image
		https://chrome.google.com/webstore/detail/view-image/jpcmhcelnjdmblfmjabdeclccemkghjk/related
		https://github.com/bijij/ViewImage
		
		In case if you are using google search and wanted to save the images directly. Also shame on
		gettyimages for stupid reasons to force google to remove that button, they said that it:
			Bypass advertisements - Why ban the act of providing links to bypass ads when there are
			ad blockers and pi-hole?
			
			Easy to steal images - then why didn't you put the premium images behind a robots.txt?
Other tools
	Command-line-based scripts
		Links extractor, by DevHarsh
		https://github.com/devharsh/Links-Extractor
		
		This script extracts links of a given web page. Syntax:
			python extractor.py "https://example.com" >>OutputLog.txt
		This is useful if you have lots of individual pages containing links to extract.
		Warning: This only extract the base HTML contents without any javascript code, therefore if any
		content that was generated by javascript contains a link, those won't be caught in the links extractor.